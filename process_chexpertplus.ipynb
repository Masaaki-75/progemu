{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on processing CheXpertPlus dataset\n",
    "\n",
    "Due to the [license](https://stanfordaimi.azurewebsites.net/datasets/5158c524-d3ab-4e02-96e9-6ee9efc110a1) of the CheXpertPlus dataset, we are unable to redistribute the original chest X-ray (CXR) images and detailed radiological reports.\n",
    "\n",
    "In this tutorial, we will show how to access and process the CheXpertPlus dataset, and link the CXRs with the textual data in our ICG-CXR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accessing CheXpertPlus Dataset\n",
    "\n",
    "The homepage of CheXpertPlus is at: https://stanfordaimi.azurewebsites.net/datasets/5158c524-d3ab-4e02-96e9-6ee9efc110a1\n",
    "\n",
    "After logging in, you can download this dataset using `AzCopy` or `Azure Storage Explorer`. The downloaded file structure should look like:\n",
    "```\n",
    "chexpertplus_data\n",
    "├── chexbert_labels.zip\n",
    "├── df_chexpert_plus_240401.csv\n",
    "├── radgraph-XL-annotations.zip\n",
    "├── DICOM\n",
    "└── PNG\n",
    "    ├── train\n",
    "    │   ├── patient00001\n",
    "    │   |   ├── study1\n",
    "    │   |   |   ├── view1_frontal.png\n",
    "    │   |   |   └── ...\n",
    "    │   |   ├── study2\n",
    "    │   |   └── ...\n",
    "    │   ├── patient00002\n",
    "    |   └── ...\n",
    "    └── valid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving CXR Data in the CheXpertPlus Dataset.\n",
    "\n",
    "Then, we link the CXRs from the CheXpertPlus dataset to the CheXpertPlus study pairs in our ICG-CXR dataset. Before we do this, let's first import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def read_json(json_path, encoding='utf-8'):\n",
    "    with open(json_path, 'r', encoding=encoding) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def write_json(data, json_path, write_mode='w', encoding='utf-8', ensure_ascii=False):\n",
    "    with open(json_path, write_mode, encoding=encoding) as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=ensure_ascii)\n",
    "\n",
    "\n",
    "def show_multiple_images(\n",
    "    images, \n",
    "    nrows=1, \n",
    "    ncols=None, \n",
    "    titles=None, \n",
    "    suptitle=None, \n",
    "    tight=True, \n",
    "    cmaps='gray', \n",
    "    figsize=None, \n",
    "    dpi=None, \n",
    "    set_axis_off=True,\n",
    "):\n",
    "\n",
    "    num_imgs = len(images)\n",
    "    ncols = num_imgs // nrows if ncols is None else ncols\n",
    "    if ncols * nrows < num_imgs:\n",
    "        ncols += 1\n",
    "    num_plots = int(nrows * ncols)\n",
    "    cmaps = [cmaps] * num_imgs if not isinstance(cmaps, (tuple, list)) else cmaps\n",
    "    titles = [titles] * num_imgs if not isinstance(titles, (tuple, list)) else titles\n",
    "    assert num_imgs <= num_plots, f'num_imgs = {num_imgs}, nrows = {nrows}, ncols = {ncols}.'\n",
    "    fig, axes = plt.subplots(nrows, ncols, squeeze=False, figsize=figsize, dpi=dpi)\n",
    "    axes = axes.flatten()\n",
    "    for i in range(num_imgs):\n",
    "        strtype = str(type(images[i]))\n",
    "        if 'torch.Tensor' in strtype:\n",
    "            img = images[i].cpu().squeeze()\n",
    "        elif 'numpy.ndarray' in strtype:\n",
    "            img = images[i].squeeze()\n",
    "        elif 'Image.Image' in strtype:\n",
    "            img = np.array(images[i]).squeeze()\n",
    "        else:\n",
    "            img = images[i]\n",
    "        \n",
    "        axes[i].imshow(img, cmap=cmaps[i])\n",
    "        if set_axis_off:\n",
    "            axes[i].axis('off')\n",
    "        if titles is not None:\n",
    "            axes[i].set_title(titles[i])\n",
    "    \n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle)\n",
    "    \n",
    "    if num_imgs < num_plots:\n",
    "        for i in range(num_imgs, num_plots):\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "    if tight:\n",
    "        fig.tight_layout()\n",
    "    \n",
    "\n",
    "def find_files_recursively(directory, ext='', inclusions='', exclusions=None):\n",
    "    matched_paths = []\n",
    "    \n",
    "    # Ensure inclusions and exclusions are lists\n",
    "    if not isinstance(inclusions, (list, tuple)):\n",
    "        inclusions = [inclusions]\n",
    "    if exclusions is not None and not isinstance(exclusions, (list, tuple)):\n",
    "        exclusions = [exclusions]\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        raise ValueError(f'Path does not exist: {directory}.')\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        filtered_dirs = []\n",
    "        \n",
    "        for d in dirs:\n",
    "            full_path = os.path.join(root, d)\n",
    "            # Only treat it as a file if it's actually a file\n",
    "            if d.endswith('.nii.gz') and os.path.isfile(full_path):\n",
    "                files.append(d)  # Treat it as a file\n",
    "            else:\n",
    "                filtered_dirs.append(d)  # Keep directories that are actually directories\n",
    "\n",
    "        dirs[:] = filtered_dirs  # Modify dirs in place to control recursion\n",
    "\n",
    "        for file in files:\n",
    "            cond1 = all(s in file for s in inclusions)\n",
    "            cond2 = file.endswith(ext)\n",
    "            cond3 = all(s not in file for s in exclusions) if exclusions else True\n",
    "\n",
    "            if cond1 and cond2 and cond3:\n",
    "                matched_paths.append(os.path.join(root, file))\n",
    "\n",
    "    return sorted(matched_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Know the ICG-CXR Data\n",
    "\n",
    "We will use the JSON files in the ICG-CXR (CheXpertPlus Ext.) dataset to retrieve the CXR images in the ChexertPlus dataset. \n",
    "\n",
    "So, first thing first, let's take a look at the JSON file in the ICG-CXR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data number in ICG-CXR (ChexPertPlus Ext.): 3655\n",
      "Patient number in ICG-CXR (ChexPertPlus Ext.): 2498\n"
     ]
    }
   ],
   "source": [
    "icgcxr_chexp_dir = './chexpertplus'\n",
    "\n",
    "# Recursively find all JSON files in the ICG-CXR (CheXpertPlus Ext.) directory\n",
    "icgcxr_chexp_meta_paths = find_files_recursively(icgcxr_chexp_dir, ext='.json')\n",
    "\n",
    "icgcxr_chexp_patients = [_.split('/')[-3] for _ in icgcxr_chexp_meta_paths]\n",
    "icgcxr_chexp_patients = sorted(list(set(icgcxr_chexp_patients)))\n",
    "print(f\"Data number in ICG-CXR (ChexPertPlus Ext.): {len(icgcxr_chexp_meta_paths)}\")\n",
    "print(f\"Patient number in ICG-CXR (ChexPertPlus Ext.): {len(icgcxr_chexp_patients)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'changes-of-findings': 'There is blunting of the left costophrenic angle, which was previously sharp, indicating the presence of a trace pleural effusion.',\n",
       " 'progression-description': 'Mild pleural effusion has developed in the left lower lung.',\n",
       " 'comment': {'confidence': 5,\n",
       "  'reason': 'Clear and consistent observations of stable findings with a newly noted mild blunting of the left costophrenic angle.'},\n",
       " 'reference-report': {'findings': 'Due to the CheXpertPlus license, we are unable to redistribute the CXR data. Please see `process_chexpertplus.ipynb` for retrieving these data from the CheXpertPlus dataset.',\n",
       "  'impression': 'Due to the CheXpertPlus license, we are unable to redistribute the CXR data. Please see `process_chexpertplus.ipynb` for retrieving these data from the CheXpertPlus dataset.'},\n",
       " 'followup-report': {'findings': 'Due to the CheXpertPlus license, we are unable to redistribute the CXR data. Please see `process_chexpertplus.ipynb` for retrieving these data from the CheXpertPlus dataset.',\n",
       "  'impression': 'Due to the CheXpertPlus license, we are unable to redistribute the CXR data. Please see `process_chexpertplus.ipynb` for retrieving these data from the CheXpertPlus dataset.'},\n",
       " 'time-interval': None,\n",
       " 'view': 'PA',\n",
       " 'subject-id': '00012',\n",
       " 'reference-study-id': '00002',\n",
       " 'followup-study-id': '00003',\n",
       " 'reference-dicom-id': 'train/patient00012/study2/view1_frontal',\n",
       " 'followup-dicom-id': 'train/patient00012/study3/view1_frontal',\n",
       " 'reference-study-order': 3,\n",
       " 'followup-study-order': 5,\n",
       " 'reference-study-index': 1,\n",
       " 'followup-study-index': 2,\n",
       " 'reference-narrative': '\\n**WARNING-NO SUMMARY CODE\\n \\n TWO VIEW CHEST:  18/28 \\n \\n ',\n",
       " 'followup-narrative': '\\nEXAM: Chest 2 Views, 8/3/2002\\n \\n',\n",
       " 'age': 61.0,\n",
       " 'sex': 'Female',\n",
       " 'race': 'Other',\n",
       " 'ethnicity': 'Non-Hispanic/Non-Latino',\n",
       " 'recent_bmi': 30.3,\n",
       " 'deceased': 'Yes',\n",
       " 'split': 'train',\n",
       " 'reg_metric': 2165.8680753770136,\n",
       " 'reference-diseases': {'Enlarged Cardiomediastinum': -100,\n",
       "  'Cardiomegaly': -100,\n",
       "  'Lung Opacity': -100,\n",
       "  'Lung Lesion': -100,\n",
       "  'Edema': -100,\n",
       "  'Consolidation': -100,\n",
       "  'Pneumonia': -100,\n",
       "  'Atelectasis': -100,\n",
       "  'Pneumothorax': -100,\n",
       "  'Pleural Effusion': -100,\n",
       "  'Pleural Other': -100,\n",
       "  'Fracture': -100,\n",
       "  'Support Devices': -100,\n",
       "  'No Finding': 1.0},\n",
       " 'followup-diseases': {'Enlarged Cardiomediastinum': -100,\n",
       "  'Cardiomegaly': -100,\n",
       "  'Lung Opacity': -100,\n",
       "  'Lung Lesion': -100,\n",
       "  'Edema': -100,\n",
       "  'Consolidation': -100,\n",
       "  'Pneumonia': -100,\n",
       "  'Atelectasis': -100,\n",
       "  'Pneumothorax': -100,\n",
       "  'Pleural Effusion': -100,\n",
       "  'Pleural Other': -100,\n",
       "  'Fracture': -100,\n",
       "  'Support Devices': -100,\n",
       "  'No Finding': 1.0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_example = read_json(icgcxr_chexp_meta_paths[0])\n",
    "data_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Retrieve CXR Images\n",
    "\n",
    "Now let's retrieve the CXRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexp_png_root = '/your/path/to/chexpertplus_data'\n",
    "chexp_png_dir = os.path.join(chexp_png_root, 'PNG')\n",
    "\n",
    "for path in tqdm(icgcxr_chexp_meta_paths):\n",
    "    meta = read_json(path)\n",
    "    \n",
    "    # This is the image identifier of the prior image in a pair of consecutive studies:\n",
    "    ref_img_id = meta['reference-dicom-id']  \n",
    "    # This is the image identifier of the subsequent image in a pair of consecutive studies:\n",
    "    flu_img_id = meta['followup-dicom-id']\n",
    "    \n",
    "    # Source image paths\n",
    "    src_ref_img_path = os.path.join(chexp_png_dir, ref_img_id + '.png')\n",
    "    src_flu_img_path = os.path.join(chexp_png_dir, flu_img_id + '.png')\n",
    "    \n",
    "    # Target image paths\n",
    "    tgt_ref_img_path = path.replace('.json', '-ref-init.png')\n",
    "    tgt_flu_img_path = path.replace('.json', '-flu-init.png')\n",
    "    \n",
    "    # Retreive the CXRs\n",
    "    shutil.copy(src_ref_img_path, tgt_ref_img_path)\n",
    "    shutil.copy(src_flu_img_path, tgt_flu_img_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Retrieve CXR Reports (optional)\n",
    "\n",
    "Since ICG-CXR dataset directly provides the disease progression prompts and the radiological differences between two consectuive CXR images in a study pair, this step is optional.\n",
    "\n",
    "But if you would like to play with the original text data in CheXpertPlus dataset, you can run the following code to retrieve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexp_report_path = os.path.join(chexp_png_root, 'df_chexpert_plus_240401.csv')\n",
    "chexp_reports = pd.read_csv(chexp_report_path, usecols=['path_to_dcm', 'section_findings', 'section_impression'])\n",
    "\n",
    "# We need to reorganize the `chexp_reports`, so we can use the image identifier to find the corresponding report.\n",
    "chexp_reports_reorg = {}\n",
    "for idx, row in chexp_reports.iterrows():\n",
    "    path_to_dcm = row['path_to_dcm']\n",
    "    # `path_to_dcm` is a string like `train/patient42142/study5/view1_frontal.dcm`\n",
    "    img_identifier = path_to_dcm.replace('.dcm', '')\n",
    "    chexp_reports_reorg[img_identifier] = {\n",
    "        'findings': row['section_findings'],\n",
    "        'impression': row['section_impression']\n",
    "    }\n",
    "\n",
    "# We then can retrieve the textual data.\n",
    "for path in tqdm(icgcxr_chexp_meta_paths):\n",
    "    meta = read_json(path)\n",
    "    \n",
    "    ref_img_id = meta['reference-dicom-id']\n",
    "    ref_findings = str(chexp_reports_reorg[ref_img_id]['findings'])\n",
    "    ref_impression = str(chexp_reports_reorg[ref_img_id]['impression'])\n",
    "    \n",
    "    flu_img_id = meta['followup-dicom-id']\n",
    "    flu_findings = str(chexp_reports_reorg[flu_img_id]['findings'])\n",
    "    flu_impression = str(chexp_reports_reorg[flu_img_id]['impression'])\n",
    "    \n",
    "    meta['reference-report'] = {'findings': ref_findings, 'impression': ref_impression}\n",
    "    meta['followup-report'] = {'findings': flu_findings, 'impression': flu_impression}\n",
    "    \n",
    "    write_json(meta, path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Registration\n",
    "\n",
    "The registration is performed using the SimpleITK library. The code is somehow complex and tedious; it is also time-consuming unless run with multiple processes. So we are not putting it in this Jupyter notebook. Please see `register_chexpertplus.py` for details.\n",
    "\n",
    "However, some images may be poorly registered and need to be manually fixed. In this case, we recommend using Jupyter notebook to fix those images in an interactive way. \n",
    "\n",
    "For more information, please contact clma24@m.fudan.edu.cn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
